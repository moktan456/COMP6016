{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moktan456/Data-Mining/blob/main/05_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdkTC_K1K6Ka"
      },
      "source": [
        "# Practical 05 - Data Classification\n",
        "\n",
        "This practical is designed to be run over two weeks.\n",
        "You are encouraged to complete as much of the prac as you can before the second week so that we can spend time addressing questions and problems that people have.\n",
        "\n",
        "As usual, you should save a copy of this notebook in Google drive (or on your own system if not using Colaboratory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFHFC-0WL4r2"
      },
      "source": [
        "# Q1 Iris classification\n",
        "We will begin by running some classification algorithms on the Iris data set.\n",
        "This is one of the most commonly used machine learning datasets for classification. The data can be found at the [UCI repository](https://archive.ics.uci.edu/ml/datasets/iris), however it's small size and large popularity means that many machine learning libraries are bundled with the data.\n",
        "\n",
        "In this task you will explore some of the basics of data classification using [scikit-learn](https://scikit-learn.org/stable/index.html) (`sklearn`).\n",
        "\n",
        "1. Import the Iris data from `sklearn`. Read the documenation that describes the data, what the attributes are, and what the classification task is.\n",
        "1. Split the data into two subsets:\n",
        "  - A training subset comprising 75% of the data\n",
        "  - A testing subset comprising 25% of the data\n",
        "1. Using the k-NN classifier (`sklearn.neighbors.NearestNeighbors`):\n",
        "  - Train the classifier with the following options and record the error rate:\n",
        "    - `weights = uniform` or `distance`\n",
        "    - `k = 1, 3, 7, 11, 17,` or `21`\n",
        "  - Of the 12 combinations of the above, choose the one with the lowest error rate as your *champion*.\n",
        "  - Train your *champoin* using the entire training data-set, and evaluate it on the test test.\n",
        "  - Create a confusion matrix by comparing the predicted and actual classes for the test data.\n",
        "1. Using a descision tree classifier:\n",
        " - Train the classifier using both the `Gini index` and `entropy` criterion for splitting.\n",
        " - Choose the classifier which has the highest F1 score as your best classifier.\n",
        " - Plot the descision tree for your best classifier.\n",
        "1. Using a naive-Bayes clasffieir:\n",
        " - Train the classifier on all the training data.\n",
        " - Predict the classes of the test data.\n",
        " - Plot a confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRVdUMwTbdB-"
      },
      "source": [
        "## Import the Iris data from sklearn.\n",
        "Read the documenation that describes the data, what the attributes are, and what the classification task is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTPxSgWJL3nI"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hStz8KQqTZI7"
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "# Inspect the data structure\n",
        "iris.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the description to learn more about the data set\n",
        "print(iris['DESCR'])"
      ],
      "metadata": {
        "id": "qWcNPTWF5S18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZQZohuVb5Yf"
      },
      "source": [
        "## Split the data into two subsets\n",
        " Split the data into two subsets:\n",
        "  - A training subset comprising 75% of the data\n",
        "  - A testing subset comprising 25% of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryPCCpo_gTqx"
      },
      "source": [
        "# Normally our we are given train/test data separately\n",
        "# hewever for this prac we will take 25% of the iris data can pretend that it's test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(?, ?, # this should be the data matrix and the class labels\n",
        "                                                    test_size=?, # use a test size of 25%\n",
        "                                                    random_state=4) # this random state ensures that we get the same subset each time we call this cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "rGgFgsF05zza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbGjUDjcJnZ"
      },
      "source": [
        "## Explore different ways to split data for cross validation\n",
        "\n",
        "sklearn provides three methods to divide data into train/test sets:\n",
        "- ShuffleSplit\n",
        "  - Random sampling\n",
        "- Kfold\n",
        "  - Ordered sampling\n",
        "- StratifiedKFold\n",
        "  - Stratified sampling\n",
        "\n",
        "Use each of the above methods to create a 10 fold split of the data for cross validation and visualise the splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNqVRkGkcOt_"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold, ShuffleSplit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFvx3hGycShi"
      },
      "source": [
        "# This is random sampling\n",
        "ss = ShuffleSplit(n_splits=10, test_size=15, random_state=4)\n",
        "# This is non-random sampling, we just break the data in to 10 contiguous sub-sets\n",
        "kf = KFold(n_splits=10)\n",
        "# Ensuring the balance between classes in the model/validate sets\n",
        "# means we should use stratified sampling\n",
        "skf = StratifiedKFold(n_splits=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLaQ7bfHdLM-"
      },
      "source": [
        "# This cell sets up a nice visulisation that I found on the scikit-learn documentation page.\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "cmap_data = plt.cm.Paired\n",
        "cmap_cv = plt.cm.coolwarm\n",
        "\n",
        "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
        "    \"\"\"\n",
        "    Create a sample plot for indices of a cross-validation object.\n",
        "    Adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#define-a-function-to-visualize-cross-validation-behavior\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    cv: cross validation method\n",
        "\n",
        "    X : training data\n",
        "\n",
        "    y : data labels\n",
        "\n",
        "    group : group labels\n",
        "\n",
        "    ax : matplolib axes object\n",
        "\n",
        "    n_splits : number of splits\n",
        "\n",
        "    lw : line width for plotting\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the training/testing visualizations for each CV split\n",
        "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
        "        # Fill in indices with the training/test groups\n",
        "        indices = np.array([np.nan] * len(X))\n",
        "        indices[tt] = 1\n",
        "        indices[tr] = 0\n",
        "\n",
        "        # Visualize the results\n",
        "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
        "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
        "                   vmin=-.2, vmax=1.2)\n",
        "\n",
        "    # Plot the data classes and groups at the end\n",
        "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
        "               c=y, marker='_', lw=lw, cmap=cmap_data)\n",
        "\n",
        "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
        "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
        "\n",
        "    # Formatting\n",
        "    yticklabels = list(range(n_splits)) + ['class', 'group']\n",
        "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
        "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
        "           ylim=[n_splits+2.2, -.2])\n",
        "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJA_NsnmdbD9"
      },
      "source": [
        "# Set up a figure with three subplots\n",
        "fig, ax = plt.subplots(1,3, figsize=(18,6))\n",
        "# visualise the ShulffleSplit algorithm\n",
        "plot_cv_indices(ss,\n",
        "                X,\n",
        "                y,\n",
        "                group=None,\n",
        "                ax=ax[0],\n",
        "                n_splits=10)\n",
        "# visualise the KFolds algorithm\n",
        "plot_cv_indices(kf,\n",
        "                X,\n",
        "                y,\n",
        "                group=None,\n",
        "                ax=ax[1],\n",
        "                n_splits=10)\n",
        "# visualise the StratifiedKFolds algorithm\n",
        "plot_cv_indices(skf,\n",
        "                X,\n",
        "                y,\n",
        "                group=None,\n",
        "                ax=ax[2],\n",
        "                n_splits=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look at the above figure and note the following:\n",
        "- The horizontal bars represent the 150 instances in our data set, with thier index shown on the horizontal axis.\n",
        "- The vertical axis shows different cross validation iterations, plus an visual indicator of the class for each instance.\n",
        "  - The blue color indicates training data, while orange represents test data. Not how this changes between the three splitting methods.\n",
        "  - There are three classes of equal number, so we have three equal length bars in the second to last row. The data are sorted so that the first 50 instances are all of class 0, etc..\n",
        "- Ignore the \"group\" row, it's not useful here."
      ],
      "metadata": {
        "id": "ZC7gqqPa4PJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above figre, decide which splitting algorithm is likely to give us the best results."
      ],
      "metadata": {
        "id": "K6HeuUjw5dUz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB2JaOVIeVAF"
      },
      "source": [
        "## Use the k-NN classifier\n",
        "Using the k-NN classifier (`sklearn.neighbors.NearestNeighbors`):\n",
        "  - Train the classifier with the following options and record the error rate:\n",
        "    - `weights = uniform` or `distance`\n",
        "    - `n_neighbors = 1, 3, 7, 11, 17,` or `21`\n",
        "  - Of the 12 combinations of the above, choose the one with highest accuracy as your *champion*.\n",
        "  - Create a confusion matrix by comparing the predicted and actual classes for the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ChrJcSTe1hG"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpS2c19lh_wD"
      },
      "source": [
        "# Create a dictionary of all the parameters we'll be iterating over\n",
        "parameters = {'weights': (?,?), # this should be the different weighting schemes\n",
        "              'n_neighbors':[?]} # this should be a list of the nearest neigbhours\n",
        "# make a classifier object\n",
        "knn = KNeighborsClassifier()\n",
        "# create a GridSearchCV object to do the training with cross validation\n",
        "gscv = GridSearchCV(estimator=knn,\n",
        "                    param_grid=parameters,\n",
        "                    cv=?,  # the cross validation folding pattern\n",
        "                    scoring='accuracy')\n",
        "# now train our model\n",
        "best_knn = gscv.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBtLvYSYi6Zh"
      },
      "source": [
        "best_knn.best_params_, best_knn.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighboursClassifier(weights = best_nkk.best_params_['weights'],\n",
        "                            n_neightbours ="
      ],
      "metadata": {
        "id": "F2igZAvGB8kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKsZu86tiJS1"
      },
      "source": [
        "fig, ax = plt.subplots(1,1, figsize=(6, 6))\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(best_knn,\n",
        "                                      X_test, y_test,\n",
        "                                      display_labels=iris['target_names'],\n",
        "                                      ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-M-1XPMDzMN"
      },
      "source": [
        "## Inspect the splitting schemes\n",
        "In the previous plot we found that the test data set had unbalanced classes, even though the input data has a even ratio of three classes.\n",
        "This is because our initial split of test/train data was done without reguard to the class labels.\n",
        "\n",
        "Now we will explore the effect of different splitting schemes on the training of our data.\n",
        "We'll split the data using ShuffleSplit, KFolds, and StratifiedKFolds, and see how that affects the training of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_YGU5Zk6Hfg"
      },
      "source": [
        "fig, ax = plt.subplots(2,5, figsize=(18,6))\n",
        "# remake this object so that we get back to the same random initial state\n",
        "ss = ShuffleSplit(n_splits=10, test_size=15, random_state=4)\n",
        "print(\"Using ShuffleSplit\")\n",
        "for i, (model, validate) in enumerate(ss.split(X, y)):\n",
        "  knn = KNeighborsClassifier(n_neighbors=3, weights='uniform')\n",
        "  classifier = knn.fit(X[model], y[model])\n",
        "  ConfusionMatrixDisplay.from_estimator(classifier,\n",
        "                                        X[validate], y[validate],\n",
        "                                        display_labels=iris['target_names'],\n",
        "                                        ax=ax.ravel()[i])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8Tuyzr98Mid"
      },
      "source": [
        "fig, ax = plt.subplots(2,5, figsize=(18,6))\n",
        "\n",
        "print(\"Using KFolds\")\n",
        "for i, (model, validate) in enumerate(kf.split(X, y)):\n",
        "  knn = KNeighborsClassifier(n_neighbors=1)\n",
        "  classifier = knn.fit(X[model], y[model])\n",
        "  ConfusionMatrixDisplay.from_estimator(classifier,\n",
        "                                        X[validate], y[validate],\n",
        "                                        display_labels=iris['target_names'],\n",
        "                                        ax=ax.ravel()[i])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICgQAly48j9k"
      },
      "source": [
        "fig, ax = plt.subplots(2,5, figsize=(18,6))\n",
        "\n",
        "print(\"Using StratifiedKFolds\")\n",
        "for i, (model, validate) in enumerate(skf.split(X, y)):\n",
        "  knn = KNeighborsClassifier(n_neighbors=1)\n",
        "  classifier = knn.fit(X[model], y[model])\n",
        "  ConfusionMatrixDisplay.from_estimator(classifier,\n",
        "                                        X[validate], y[validate],\n",
        "                                        display_labels=iris['target_names'],\n",
        "                                        ax=ax.ravel()[i])\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULx_V539jW-q"
      },
      "source": [
        "## Use a descision tree classifier\n",
        "Using a descision tree classifier:\n",
        " - Train the classifier using both the `Gini index` and `entropy` criterion for splitting, and a range of `min_samples_split` between 3 and 20.\n",
        " - Choose the classifier which has the highest accuracy score as your best classifier.\n",
        " - Plot the descision tree for your best classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnZvYvPIjhHr"
      },
      "source": [
        "from sklearn import tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLbxWiLJkdqV"
      },
      "source": [
        "# Create a dictionary of all the parameters we'll be iterating over\n",
        "parameters = {'criterion': (?,?),  # this should be the different splitting criteria\n",
        "              'min_samples_split':[?]} # this should be the different values for min_samples_split\n",
        "dtc = tree.DecisionTreeClassifier()\n",
        "gscv = GridSearchCV(estimator=dtc,\n",
        "                    param_grid=parameters,\n",
        "                    cv=5,\n",
        "                    scoring='accuracy')\n",
        "best_dtc = gscv.fit(X_train, y_train)\n",
        "best_dtc.best_params_, best_dtc.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNPHgbGVkUC1"
      },
      "source": [
        "fig, ax = plt.subplots(1,1, figsize=(12,12))\n",
        "tree.plot_tree(best_dtc.best_estimator_,\n",
        "               filled=True, # color the nodes based on class/purity\n",
        "               ax=ax, fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od7hXKDrlhB4"
      },
      "source": [
        "fig, ax = plt.subplots(1,1, figsize=(6, 6))\n",
        "\n",
        "ConfusionMatrixDisplay.from_estimator(best_dtc,\n",
        "                                      X_test, y_test,\n",
        "                                      display_labels=iris['target_names'],\n",
        "                                      ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ragXYCmmGI"
      },
      "source": [
        "## Use a naive-Bayes clasffieir\n",
        "Using a naive-Bayes clasffieir:\n",
        " - Train the classifier on all the training data.\n",
        " - Predict the classes of the test data.\n",
        " - Plot a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbE9ZxHUm0Gy"
      },
      "source": [
        "from sklearn import naive_bayes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTyowE_um6KU"
      },
      "source": [
        "# no parameters to adjust so no need to optimise, just train\n",
        "fig, ax = plt.subplots(1,1)\n",
        "nb = naive_bayes.GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "ConfusionMatrixDisplay.from_estimator(nb,\n",
        "                                      X_test, y_test,\n",
        "                                      display_labels=iris['target_names'],\n",
        "                                      ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rQ6XWaXrH_8"
      },
      "source": [
        "# Helpful tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT2btAKBFhfu"
      },
      "source": [
        "## Corner plot\n",
        "\n",
        "A useful plot for visualising multi-dimensional data is the corner-plot or pair plot.\n",
        "There is a function built into pandas called `scatter_matrix`, and the plotting package `seaborn` also has a function called `pairplot`.\n",
        "Let's have a look at them below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbrL8ZRn81YV"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5C9acLL-hC5"
      },
      "source": [
        "# Load the data and labels as data frams and then join them to make a new one\n",
        "df1 = pd.DataFrame(X, columns=iris.feature_names)\n",
        "df2 = pd.DataFrame(y, columns=['class'])\n",
        "df = df1.join(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "E8uHSman_7P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgBwoiHbGGPd"
      },
      "source": [
        "pd.plotting.scatter_matrix(df1,c=df['class'], figsize=(15, 15), marker='o',\n",
        "                                 hist_kwds={'bins': 20}, s=60, alpha=.8)\n",
        "print('Plotted with pandas')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sy6FVkB-wdZ"
      },
      "source": [
        "sns.pairplot(df, hue='class', palette=sns.color_palette('colorblind',3))\n",
        "print(\"Plotted with seaborn\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUuN8K0TJKFJ"
      },
      "source": [
        "The thing that I most prefer about the seaborn plot is that the diagonal entries are still separated by class.\n",
        "From this plot it is clear that the last two features are good at separating the three classes, where as the first two attributes are not so useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5q3vlNqho_M"
      },
      "source": [
        "## Correlation plot\n",
        "\n",
        "A correlation matrix is simlar to the corner plot above but it simply reports the correlation between each of the attributes.\n",
        "\n",
        "We can compute the correlation matrix using pandas with the `df.corr()` method, and the plot using either `matplotlib` or `seaborn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XhwwefQh5GX"
      },
      "source": [
        "# compute correlation matrix\n",
        "cor = df.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVHSXuAEozcD"
      },
      "source": [
        "# plot the covariance with matplotlib\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
        "im = ax.imshow(cor)\n",
        "cb = plt.colorbar(ax=ax, mappable=im)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6_1YZaCo2x4"
      },
      "source": [
        "# use seaborn to do the plot\n",
        "sns.heatmap(df.corr(), annot=True, cmap=plt.cm.Reds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAq6yxzlVdfs"
      },
      "source": [
        "Looking at the correlation plot we can see that the petal length/width are highly correlated with the class attribute and are likely useful attributes.\n",
        "The fact that they are also highly correlated with each other means that we might be able to use just one of the two features.\n",
        "\n",
        "The sepal width has much lower correlation and so is probably not so useful."
      ]
    }
  ]
}